{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing MDFs with Pony ORM.\n",
    "## MySQL\n",
    "\n",
    "SQLlite is a good tool for development, but it does not scale well.\n",
    "\n",
    "In addition to SQLLite PonyORM supports PostgreSQL, MySQL, and Oracle.\n",
    "\n",
    "This example picks up from the SQLLite example but adds MySQL support.\n",
    "\n",
    "Using a MySQL (or PostreSQL/Oracle) backend will allow for distributed processing on any node that can access the s3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: ['mdfbucket-0', 'mdfbucket-1', 'mdfbucket-2', 'mdfbucket-3', 'mdfbucket-4', 'mdfbucket-5', 'mdfbucket-6', 'mdfbucket-7', 'mdfbucket-8', 'mdfbucket-9', 'test']\n"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "from asammdf import MDF\n",
    "\n",
    "import fsspec\n",
    "import os\n",
    "s3_cfg = {\n",
    "    \"key\": \"mdf_minio_access_key\",\n",
    "    \"secret\": \"mdf_minio_secret_key\",\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": \"http://minio:9000\",\n",
    "    },\n",
    "}\n",
    "fs = fsspec.filesystem(\"s3\", **s3_cfg)\n",
    "buckets = fs.ls(\"\")\n",
    "print(f\"Buckets: {fs.ls('')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Through All Files:\n",
    "\n",
    "Walk through all S3 files and find the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 2.14 GB in 975 MDF files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "mdf_paths=list()\n",
    "for bucket in fs.ls(\"\"):\n",
    "    for root, dirs, files in fs.walk(bucket):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".mf4\") or file.lower().endswith(\".mdf\"):\n",
    "                mdf_paths.append(os.path.join(root, file))\n",
    "                \n",
    "\n",
    "mdf_size = sum([fs.info(mdf_path)[\"size\"] for mdf_path in mdf_paths])\n",
    "print(f\"Indexing {mdf_size/1024**3:.2f} GB in {len(mdf_paths)} MDF files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a random MDF path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mdfbucket-5/DÃ¤sCarGmbh/DumpTruck/27c181e6-74eb-4fd7-8476-0d751bc9b5fe.mf4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(mdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pony.orm\n",
    "from pony.orm.core import EntityMeta\n",
    "from datetime import datetime\n",
    "\n",
    "pony.orm.set_sql_debug(False)\n",
    "\n",
    "db = pony.orm.Database()\n",
    "\n",
    "db.bind(\n",
    "    provider='mysql',\n",
    "    host='mysql',\n",
    "    user='mdf_indexer_user',\n",
    "    passwd='mdf_indexer_pass',\n",
    "    db='mdf_database',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db.drop_all_tables(with_all_data=True)\n",
    "except pony.orm.ERDiagramError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asammdf\n",
    "# For Local Indexing.\n",
    "class MDF(db.Entity):\n",
    "    \"\"\"MDF ORM Entity Fancy\"\"\"\n",
    "    # Filesystem Bits.\n",
    "    key = pony.orm.Required(str, unique=True,)\n",
    "    last_modified = pony.orm.Optional(datetime, volatile=True)\n",
    "    etag = pony.orm.Optional(str,)\n",
    "    size = pony.orm.Optional(int,)\n",
    "    size_mb = pony.orm.Optional(float,)\n",
    "    storage_class = pony.orm.Optional(str,)\n",
    "    type = pony.orm.Optional(str,)\n",
    "    name = pony.orm.Optional(str,)\n",
    "    \n",
    "    # Pre-calculated bits.\n",
    "    basename = pony.orm.Optional(str,)\n",
    "    product = pony.orm.Optional(str,)\n",
    "    company = pony.orm.Optional(str,)\n",
    "\n",
    "    # ASAM MDF Bits.\n",
    "    version = pony.orm.Optional(str,)\n",
    "    channels = pony.orm.Set(\"Channel\",)\n",
    "    \n",
    "    \n",
    "    # Basename.\n",
    "    basename = pony.orm.Optional(str,)\n",
    "    channels = pony.orm.Set(\"Channel\",)\n",
    "    \n",
    "    # Metadata\n",
    "    product = pony.orm.Optional(str,)\n",
    "    company = pony.orm.Optional(str,)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MDF<{self.id},{self.product},{self.company},Ch:{len(self.channels)}>\"\n",
    "\n",
    "class Channel(db.Entity):\n",
    "    \"\"\"Channel entity to represent a \n",
    "    \n",
    "    \"\"\"\n",
    "    name = pony.orm.Required(str, unique=True,)\n",
    "    mdfs = pony.orm.Set(\"MDF\",)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Channel<{self.id},{self.name}>\"\n",
    "\n",
    "def upsert(cls, get, set=None):\n",
    "    \"\"\"\n",
    "    Interacting with Pony entities.\n",
    "\n",
    "    :param cls: The actual entity class\n",
    "    :param get: Identify the object (e.g. row) with this dictionary\n",
    "    :param set: Additional fields to set if ```get``` returns nothing.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # does the object exist\n",
    "    assert isinstance(cls, EntityMeta), f\"{cls} is not a database entity\"\n",
    "\n",
    "    # if no set dictionary has been specified\n",
    "    set = set or {}\n",
    "    db.flush()\n",
    "    if not cls.exists(**get):\n",
    "        # make new object\n",
    "        return cls(**set, **get)\n",
    "    else:\n",
    "        # get the existing object\n",
    "        obj = cls.get(**get)\n",
    "        for key, value in set.items():\n",
    "            obj.__setattr__(key, value)\n",
    "        return obj\n",
    "    \n",
    "\n",
    "db.generate_mapping(create_tables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_mdf(mdf_path):\n",
    "    \"\"\" Index the mdf file itself. \"\"\"\n",
    "    info = fs.info(mdf_path)\n",
    "    # Local File\n",
    "    MDF_ = upsert(\n",
    "    cls=MDF,\n",
    "    get={\"key\": info[\"Key\"]},\n",
    "    set={\n",
    "        \"last_modified\": info[\"LastModified\"],\n",
    "        \"etag\": info[\"ETag\"],\n",
    "        \"size\": info[\"size\"],\n",
    "        \"size_mb\": info[\"size\"] / 1024 ** 2,\n",
    "        \"storage_class\": info[\"StorageClass\"],\n",
    "        \"type\": info[\"type\"],\n",
    "        \"name\": info[\"name\"],\n",
    "        \"basename\": os.path.basename(info[\"name\"])\n",
    "        },\n",
    "    )\n",
    "    try:\n",
    "        db.commit()\n",
    "        return MDF_\n",
    "    except:\n",
    "        db.rollback()\n",
    "        return None\n",
    "    \n",
    "def index_channels(mdf):\n",
    "    \"\"\"Given a MDF files, process the channels\n",
    "    \n",
    "    \"\"\"\n",
    "    # Open the MDF file.\n",
    "    with fs.open(mdf.name, \"rb\") as fid:\n",
    "        mdf_ = asammdf.MDF(fid)\n",
    "    # \n",
    "    channels=list()\n",
    "    # Loop through each of the channels in the database.\n",
    "    for channel in mdf_.channels_db.keys():\n",
    "        channel_ = upsert(Channel, {\"name\": channel})\n",
    "        channels.append(channel_)\n",
    "    MDF_ = upsert(\n",
    "    cls=MDF,\n",
    "    get={\"name\": mdf.name},\n",
    "    set={\n",
    "        \"channels\": channels\n",
    "        },\n",
    "    )\n",
    "    try:\n",
    "        db.commit()\n",
    "        return channels\n",
    "    except:\n",
    "        db.rollback()\n",
    "        return None\n",
    "        \n",
    "def index_mdf_info(mdf):\n",
    "    if isinstance(mdf, str):\n",
    "        name = mdf\n",
    "    elif isinstance(mdf, MDF):\n",
    "        name = mdf.name\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    \n",
    "    \"\"\" Index company and product information in the database from the filename.\"\"\"\n",
    "    product = os.path.basename(os.path.dirname(name))\n",
    "    company = os.path.basename(\n",
    "        os.path.dirname(\n",
    "            os.path.dirname(\n",
    "                name\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Local File\n",
    "    MDF_ = upsert(\n",
    "    cls=MDF,\n",
    "    get={\"name\": mdf.name},\n",
    "    set={\n",
    "        \"product\": product,\n",
    "        \"company\": company,\n",
    "        },\n",
    "    )\n",
    "    try:\n",
    "        db.commit()\n",
    "        return MDF_\n",
    "    except:\n",
    "        db.rollback()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one out of a hat.\n",
    "mdf_path = random.choice(mdf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the MDF file into the database.\n",
    "\n",
    "[Notice the __repr__ string isn't fully populated, the data isn't yet in the database]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDF<19,,,Ch:0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf = index_mdf(mdf_path)\n",
    "mdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the product and company name of the mdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDF<19,Bulldozer,HeavyEquipmentInc,Ch:0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_mdf_info(mdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, channels is still '0', because the channels have not been read with ```asammdf```. \n",
    "\n",
    "Write a function to open the file, read the channels with asammdf and insert them into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Channel<14,time>,\n",
       " Channel<15,engine_speed>,\n",
       " Channel<16,engine_speed_desired>,\n",
       " Channel<17,vehicle_speed>,\n",
       " Channel<18,coolant_temp>,\n",
       " Channel<19,longitude>,\n",
       " Channel<20,latitude>,\n",
       " Channel<21,power>,\n",
       " Channel<22,efficiency>,\n",
       " Channel<26,ADAS5_failure>,\n",
       " Channel<23,X>,\n",
       " Channel<24,Y>,\n",
       " Channel<25,Z>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_channels(mdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
