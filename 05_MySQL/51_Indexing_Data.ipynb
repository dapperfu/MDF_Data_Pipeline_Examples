{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing MDF Data in MySQL\n",
    "\n",
    "A cut down version with all of the database bits moved into a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 975 MDF files\n"
     ]
    }
   ],
   "source": [
    "from mdf_models import *\n",
    "\n",
    "import os\n",
    "import random\n",
    "import fsspec\n",
    "mdf_paths=list()\n",
    "\n",
    "s3_cfg = {\n",
    "    \"key\": \"mdf_minio_access_key\",\n",
    "    \"secret\": \"mdf_minio_secret_key\",\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": \"http://minio:9000\",\n",
    "    },\n",
    "}\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", **s3_cfg)\n",
    "for bucket in fs.ls(\"\"):\n",
    "    for root, dirs, files in fs.walk(bucket):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".mf4\") or file.lower().endswith(\".mdf\"):\n",
    "                mdf_paths.append(os.path.join(root, file))\n",
    "print(f\"Found {len(mdf_paths)} MDF files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_mdf(mdf_path):\n",
    "    \"\"\" Index the mdf file itself. \"\"\"\n",
    "    info = fs.info(mdf_path)\n",
    "    # Local File\n",
    "    MDF_ = upsert(\n",
    "        cls=MDF,\n",
    "        get={\"key\": info[\"Key\"]},\n",
    "        set={\n",
    "            \"last_modified\": info[\"LastModified\"],\n",
    "            \"etag\": info[\"ETag\"],\n",
    "            \"size\": info[\"size\"],\n",
    "            \"size_mb\": info[\"size\"] / 1024 ** 2,\n",
    "            \"storage_class\": info[\"StorageClass\"],\n",
    "            \"type\": info[\"type\"],\n",
    "            \"name\": info[\"name\"],\n",
    "            \"basename\": os.path.basename(info[\"name\"]),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return MDF_\n",
    "    \n",
    "def index_channels(mdf):\n",
    "    \"\"\"Given a MDF files, process the channels\n",
    "    \n",
    "    \"\"\"\n",
    "    name = mdf.name\n",
    "    # Open the MDF file.\n",
    "    with fs.open(mdf.name, \"rb\") as fid:\n",
    "        mdf_ = asammdf.MDF(fid)\n",
    "    #\n",
    "    channels = list()\n",
    "    # Loop through each of the channels in the database.\n",
    "    for channel in mdf_.channels_db.keys():\n",
    "        channel_ = upsert(Channel, {\"name\": channel})\n",
    "        channels.append(channel_)\n",
    "    MDF_ = upsert(cls=MDF, get={\"name\": name}, set={\"channels\": channels},)\n",
    "\n",
    "def index_mdf_info(mdf):\n",
    "    name = mdf.name\n",
    "    \n",
    "    \"\"\" Index company and product information in the database from the filename.\"\"\"\n",
    "    product = os.path.basename(os.path.dirname(name))\n",
    "    company = os.path.basename(os.path.dirname(os.path.dirname(name)))\n",
    "    # Local File\n",
    "    MDF_ = upsert(\n",
    "        cls=MDF,\n",
    "        get={\"name\": name},\n",
    "        set={\"product\": product, \"company\": company,},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the first 8 MDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 470, in _handle_results\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 251, in recv\n",
      "    return _ForkingPickler.loads(buf.getbuffer())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pony/orm/core.py\", line 4594, in unpickle_entity\n",
      "    cache = entity._database_._get_cache()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pony/orm/core.py\", line 855, in _get_cache\n",
      "    ): throw(TransactionError, 'db_session is required when working with the database')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pony/utils/utils.py\", line 108, in throw\n",
      "    raise exc  # Set \"pony.options.CUT_TRACEBACK = False\" to see full traceback\n",
      "pony.orm.core.TransactionError: db_session is required when working with the database\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p=Pool(8)\n",
    "p.map(index_mdf, mdf_paths[:8]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index all of the MDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=Pool(8)\n",
    "p.map(index_mdf, mdf_paths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
